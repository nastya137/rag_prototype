# RAG-система для анализа методических документов

Данный проект представляет собой прототип Retrieval-Augmented Generation (RAG) системы для ответов на вопросы по методическим документам (оформление курсовых и выпускных квалификационных работ).
Система состоит из:

- векторного хранилища Qdrant,
- модели эмбеддингов intfloat/multilingual-e5-base,
- реранкера cross-encoder/ms-marco-MiniLM-L-6-v2
- LLM (Ollama, модель mistral),
- механизма поиска релевантных чанков и генерации ответа.

Файл test_results.xlsx содержит в себе результаты тестирования проекта.

Каждый лист — отдельный эксперимент с датой и версией.

Метрики:
- Оценка работы ресивера (0-2)
- Оценка работы генератора (0-2)
- Итоговая оценка (0–4)
- Средняя точность (%)

## Архитектуры и версии системы

В проекте реализуются и сравниваются две архитектуры retrieval:

### Версия 1: Базовая RAG-архитектура (векторный поиск + реранкинг)
- Классический RAG-пайплайн на основе векторного поиска (ChromaDB).
- Используются эмбеддинги sentence-transformers.
- Применяется Cross-Encoder реранкер для переранжирования.
- Архитектура ориентирована преимущественно на извлечение справочной информации из документов.

Цель версии:
- Зафиксировать базовый уровень качества retrieval без использования графовых структур.
- Используется как baseline для последующего сравнения.

---

### Версия 2: Графовая архитектура извлечения знаний (в разработке)
- Retrieval на основе семантического графа.
- Узлы графа: фрагменты методички, разделы, типы сущностей.
- Рёбра: логические, структурные и семантические связи.
- Архитектура ориентирована на:
  - пошаговые инструкции,
  - сравнение,
  - процедурные ответы,
  - композицию информации из нескольких источников.

Цель версии:
- Показать преимущество графовой архитектуры в задачах, где простой векторный поиск даёт фрагментарные или нерелевантные ответы.


## Структура проекта

- `/documents        # исходные документы`
- `/chroma_db        # векторная база (создаётся автоматически)`
- `/model            # кэш моделей (создаётся автоматически)`
- `/src              # исходный код`

## Требования к системе
- Python 3.10+
- Установленный Ollama
- Интернет при первом запуске (для загрузки моделей Ollama и ML-весов)
- Запущенный Neo4j (bolt://127.0.0.1:7687)
- Локальный Qdrant (через QdrantClient(path=...))
- После первой инициализации проект может работать офлайн.

## Требования к документам
- Форматы: PDF, DOCX, TXT
- Язык: русский

## Использование
1. Перед запуском установите зависимости:
`pip install -r requirements.txt`
Также необходимо скачать Ollama:
`ollama pull llama3.2:latest`
2. Поместите документы в директорию `/documents`
3. Запустите `get_document.py` для построения векторной базы.
Важно: при первом запуске необходимо подключение к сети Интернет для загрузки модели эмбеддингов. После загрузки модель кэшируется локально.
4. Запустите `get_answer.py` - систему вопросов и ответов. Не запускайте этот файл до того, как будет выполнена работа `get_document.py`! После ввода вопроса появится ответ на него и набор чанков, использованных для его построения.
При первом запуске `get_answer.py` также необходимо подключение к сети Интернет для загрузки реранкера. Впоследствии он будет сохранён локально.
5. Для выхода введите `стоп`

