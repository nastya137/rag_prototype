# RAG-система для анализа методических документов

Данный проект представляет собой прототип Retrieval-Augmented Generation (RAG) системы для ответов на вопросы по методическим документам (оформление курсовых и выпускных квалификационных работ).
Система состоит из:

- векторного хранилища ChromaDB,
- модели эмбеддингов multi-qa-mpnet-base-dot-v1,
- LLM (Ollama, модель llama3.2),
- механизма поиска релевантных чанков и генерации ответа.

## Архитектуры и версии системы

В проекте реализуются и сравниваются две архитектуры retrieval:

### Версия 1: Базовая RAG-архитектура (векторный поиск + реранкинг)
- Классический RAG-пайплайн на основе векторного поиска (ChromaDB).
- Используются эмбеддинги sentence-transformers.
- Применяется Cross-Encoder реранкер для переранжирования.
- Архитектура ориентирована преимущественно на извлечение справочной информации из документов.

Цель версии:
- Зафиксировать базовый уровень качества retrieval без использования графовых структур.
- Используется как baseline для последующего сравнения.

Ветка репозитория:
- `rag-system-baseline`

---

### Версия 2: Графовая архитектура извлечения знаний (в разработке)
- Retrieval на основе семантического графа.
- Узлы графа: фрагменты методички, разделы, типы сущностей.
- Рёбра: логические, структурные и семантические связи.
- Архитектура ориентирована на:
  - пошаговые инструкции,
  - сравнение,
  - процедурные ответы,
  - композицию информации из нескольких источников.

Цель версии:
- Показать преимущество графовой архитектуры в задачах, где простой векторный поиск даёт фрагментарные или нерелевантные ответы.

Ветка репозитория:
- `main`

## Структура проекта

- `/documents        # исходные документы`
- `/chroma_db        # векторная база (создаётся автоматически)`
- `/model            # кэш моделей (создаётся автоматически)`
- `/src              # исходный код`

## Требования к системе
- Python 3.10+
- Установленный Ollama
- Загруженная модель: `ollama pull llama3.2`

## Требования к документам
- Форматы: PDF, DOCX, TXT
- Язык: русский

## Использование
1. Перед запуском установите зависимости:
`pip install -r requirements.txt`
Также необходимо скачать Ollama:
`ollama pull llama3.2:latest`
2. Поместите документы в директорию `/documents`
3. Запустите `get_document.py` для построения векторной базы.
Важно: при первом запуске необходимо подключение к сети Интернет для загрузки модели эмбеддингов. После загрузки модель кэшируется локально.
4. Запустите `get_answer.py` - систему вопросов и ответов. После ввода вопроса появится ответ на него и набор чанков, использованных для его построения.
При первом запуске `get_answer.py` также необходимо подключение к сети Интернет для загрузки реранкера. Впоследствии он будет сохранён локально.
5. Для выхода введите `стоп`.
